{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "63e6ca50",
   "metadata": {},
   "source": [
    "STEP 1\n",
    "\n",
    "- get frequency of words\n",
    "- fusion wit the databses containing the translations\n",
    "\n",
    "STEP 2\n",
    "\n",
    "- front \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8c6fe26",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Extracting & cleaning occurence / words with filter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fc32521",
   "metadata": {
    "hidden": true
   },
   "source": [
    "**cleaning rapide avec scipy/nltk?**\n",
    "\n",
    "- Filter with the words of the english language : http://www.mieliestronk.com/wordlist.html\n",
    "- stopword, numbers, names, special charaters, remove foreign words\n",
    "- split les mots en deux avec un ' au milieu\n",
    "\n",
    "**Fusion with the dataset containing french & english words**\n",
    "\n",
    "- how to avoid a infinite runtime ?? \n",
    "- tokenization : https://machinelearningmastery.com/clean-text-machine-learning-python/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "9f2d8743",
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mC:\\Users\\JOSEPH~1.PIC\\AppData\\Local\\Temp/ipykernel_34092/2826215563.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;31m# StopWords\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdownload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\nltk\\downloader.py\u001b[0m in \u001b[0;36mdownload\u001b[1;34m(self, info_or_id, download_dir, quiet, force, prefix, halt_on_error, raise_on_error, print_error_to)\u001b[0m\n\u001b[0;32m    761\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mdownload_dir\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    762\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_download_dir\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdownload_dir\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 763\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_interactive_download\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    764\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    765\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\nltk\\downloader.py\u001b[0m in \u001b[0;36m_interactive_download\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1111\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mTKINTER\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1112\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1113\u001b[1;33m                 \u001b[0mDownloaderGUI\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmainloop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1114\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mTclError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1115\u001b[0m                 \u001b[0mDownloaderShell\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\nltk\\downloader.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, dataserver, use_threads)\u001b[0m\n\u001b[0;32m   1408\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_init_menu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1409\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1410\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fill_table\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1411\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mHTTPError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1412\u001b[0m             \u001b[0mshowerror\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Error reading from server\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\nltk\\downloader.py\u001b[0m in \u001b[0;36m_fill_table\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1743\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1744\u001b[0m             \u001b[1;32massert\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"bad tab value %r\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_tab\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1745\u001b[1;33m         \u001b[0mrows\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_package_to_columns\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mitem\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mitems\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1746\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_table\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrows\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1747\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\nltk\\downloader.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m   1743\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1744\u001b[0m             \u001b[1;32massert\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"bad tab value %r\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_tab\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1745\u001b[1;33m         \u001b[0mrows\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_package_to_columns\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mitem\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mitems\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1746\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_table\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrows\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1747\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\nltk\\downloader.py\u001b[0m in \u001b[0;36m_package_to_columns\u001b[1;34m(self, pkg)\u001b[0m\n\u001b[0;32m   1899\u001b[0m                 \u001b[0mrow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpkg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mid\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1900\u001b[0m             \u001b[1;32melif\u001b[0m \u001b[0mcolumn_name\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"Status\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1901\u001b[1;33m                 \u001b[0mrow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_ds\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstatus\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpkg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1902\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1903\u001b[0m                 \u001b[0mattr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcolumn_name\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\" \"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"_\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\nltk\\downloader.py\u001b[0m in \u001b[0;36mstatus\u001b[1;34m(self, info_or_id, download_dir)\u001b[0m\n\u001b[0;32m    862\u001b[0m         \u001b[1;31m# Handle collections:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    863\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minfo\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mCollection\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 864\u001b[1;33m             \u001b[0mpkg_status\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstatus\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpkg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mid\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mpkg\u001b[0m \u001b[1;32min\u001b[0m \u001b[0minfo\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpackages\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    865\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSTALE\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mpkg_status\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    866\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSTALE\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\nltk\\downloader.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    862\u001b[0m         \u001b[1;31m# Handle collections:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    863\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minfo\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mCollection\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 864\u001b[1;33m             \u001b[0mpkg_status\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstatus\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpkg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mid\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mpkg\u001b[0m \u001b[1;32min\u001b[0m \u001b[0minfo\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpackages\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    865\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSTALE\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mpkg_status\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    866\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSTALE\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\nltk\\downloader.py\u001b[0m in \u001b[0;36mstatus\u001b[1;34m(self, info_or_id, download_dir)\u001b[0m\n\u001b[0;32m    881\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    882\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0minfo\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mid\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_status_cache\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 883\u001b[1;33m                     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_status_cache\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0minfo\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mid\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_pkg_status\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minfo\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfilepath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    884\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_status_cache\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0minfo\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mid\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    885\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\nltk\\downloader.py\u001b[0m in \u001b[0;36m_pkg_status\u001b[1;34m(self, info, filepath)\u001b[0m\n\u001b[0;32m    897\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    898\u001b[0m         \u001b[1;31m# Check if the file's checksum matches\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 899\u001b[1;33m         \u001b[1;32mif\u001b[0m \u001b[0mmd5_hexdigest\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0minfo\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mchecksum\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    900\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSTALE\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    901\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\nltk\\downloader.py\u001b[0m in \u001b[0;36mmd5_hexdigest\u001b[1;34m(file)\u001b[0m\n\u001b[0;32m   2207\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2208\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"rb\"\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0minfile\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2209\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0m_md5_hexdigest\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minfile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2210\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0m_md5_hexdigest\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2211\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\nltk\\downloader.py\u001b[0m in \u001b[0;36m_md5_hexdigest\u001b[1;34m(fp)\u001b[0m\n\u001b[0;32m   2214\u001b[0m     \u001b[0mmd5_digest\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmd5\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2215\u001b[0m     \u001b[1;32mwhile\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2216\u001b[1;33m         \u001b[0mblock\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1024\u001b[0m \u001b[1;33m*\u001b[0m \u001b[1;36m16\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# 16k blocks\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2217\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mblock\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2218\u001b[0m             \u001b[1;32mbreak\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('punkt')\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "\n",
    "# StopWords\n",
    "# nltk.download()\n",
    "stop_words = stopwords.words('english')\n",
    "# print(stop_words)\n",
    "\n",
    "\n",
    "# Special characters\n",
    "List_special = []\n",
    "for caracter in '!\"#$%&()*+,-./:;<=>?@[\\]^_`{|}~\"' :\n",
    "    List_special.append(caracter)\n",
    "    \n",
    "    \n",
    "# List of english words\n",
    "with open(\"english_language_lowercase.txt\", 'r', encoding='UTF-8') as file:\n",
    "    lines_english_language = file.readlines()\n",
    "    lines_english_language = [line.rstrip() for line in lines_english_language]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "8df1c329",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████| 2184780/2184780 [20:28<00:00, 1778.38it/s]\n"
     ]
    }
   ],
   "source": [
    "filename = \"enwiki-20190320-words-frequency.txt\"\n",
    "\n",
    "with open(filename, 'r', encoding='UTF-8') as file:\n",
    "    lines = file.readlines()\n",
    "    lines = [line.rstrip() for line in tqdm(lines) if line.split(\" \")[0] in lines_english_language]\n",
    "\n",
    "# new_list = [line.split(\" \") for line in lines]\n",
    "liste_words = [line.split(\" \")[0] for line in lines]\n",
    "liste_occurences = [line.split(\" \")[1] for line in lines]\n",
    "\n",
    "df = pd.DataFrame({\"words\" : liste_words,\n",
    "                 \"occurences\" : liste_occurences})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "30d0c406",
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>words</th>\n",
       "      <th>occurences</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2184730</th>\n",
       "      <td>betwodded</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2184731</th>\n",
       "      <td>eugenies</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2184732</th>\n",
       "      <td>kntv-produced</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2184733</th>\n",
       "      <td>white-flannelled</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2184734</th>\n",
       "      <td>much-interrupted</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2184735</th>\n",
       "      <td>artaris</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2184736</th>\n",
       "      <td>stigmoid</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2184737</th>\n",
       "      <td>alataq</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2184738</th>\n",
       "      <td>clp-held</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2184739</th>\n",
       "      <td>chinese-localized</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2184740</th>\n",
       "      <td>braguda's</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2184741</th>\n",
       "      <td>ausdrucksvoll</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2184742</th>\n",
       "      <td>benihana's</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2184743</th>\n",
       "      <td>oblena</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2184744</th>\n",
       "      <td>arulalarkal</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2184745</th>\n",
       "      <td>eadu-vasippu</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2184746</th>\n",
       "      <td>daanglungsod</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2184747</th>\n",
       "      <td>v-hire</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2184748</th>\n",
       "      <td>caraccilo</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2184749</th>\n",
       "      <td>cahayagan</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2184750</th>\n",
       "      <td>panaytayon</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2184751</th>\n",
       "      <td>paningayan</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2184752</th>\n",
       "      <td>telegrapo</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2184753</th>\n",
       "      <td>paroco</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2184754</th>\n",
       "      <td>education-division</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2184755</th>\n",
       "      <td>libod</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2184756</th>\n",
       "      <td>helloworldservice</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2184757</th>\n",
       "      <td>cytodifferentiation</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2184758</th>\n",
       "      <td>hinipaan</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2184759</th>\n",
       "      <td>binogawan</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2184760</th>\n",
       "      <td>pinamihagan</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2184761</th>\n",
       "      <td>asfrid</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2184762</th>\n",
       "      <td>chnuba</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2184763</th>\n",
       "      <td>tryggvasson's</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2184764</th>\n",
       "      <td>gnupa's</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2184765</th>\n",
       "      <td>kaliman's</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2184766</th>\n",
       "      <td>pagagawan</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2184767</th>\n",
       "      <td>stanfilco</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2184768</th>\n",
       "      <td>formelleza</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2184769</th>\n",
       "      <td>sikad-sikad</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2184770</th>\n",
       "      <td>uraro</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2184771</th>\n",
       "      <td>j'viens</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2184772</th>\n",
       "      <td>leytenos</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2184773</th>\n",
       "      <td>extra-burghal</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2184774</th>\n",
       "      <td>fürstenberg-blomberg</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2184775</th>\n",
       "      <td>al-ikhshīd</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2184776</th>\n",
       "      <td>石碣村</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2184777</th>\n",
       "      <td>non-number-specific</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2184778</th>\n",
       "      <td>salon-bolshevist</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2184779</th>\n",
       "      <td>fi-ta</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        words occurences\n",
       "2184730             betwodded          3\n",
       "2184731              eugenies          3\n",
       "2184732         kntv-produced          3\n",
       "2184733      white-flannelled          3\n",
       "2184734      much-interrupted          3\n",
       "2184735               artaris          3\n",
       "2184736              stigmoid          3\n",
       "2184737                alataq          3\n",
       "2184738              clp-held          3\n",
       "2184739     chinese-localized          3\n",
       "2184740             braguda's          3\n",
       "2184741         ausdrucksvoll          3\n",
       "2184742            benihana's          3\n",
       "2184743                oblena          3\n",
       "2184744           arulalarkal          3\n",
       "2184745          eadu-vasippu          3\n",
       "2184746          daanglungsod          3\n",
       "2184747                v-hire          3\n",
       "2184748             caraccilo          3\n",
       "2184749             cahayagan          3\n",
       "2184750            panaytayon          3\n",
       "2184751            paningayan          3\n",
       "2184752             telegrapo          3\n",
       "2184753                paroco          3\n",
       "2184754    education-division          3\n",
       "2184755                 libod          3\n",
       "2184756     helloworldservice          3\n",
       "2184757   cytodifferentiation          3\n",
       "2184758              hinipaan          3\n",
       "2184759             binogawan          3\n",
       "2184760           pinamihagan          3\n",
       "2184761                asfrid          3\n",
       "2184762                chnuba          3\n",
       "2184763         tryggvasson's          3\n",
       "2184764               gnupa's          3\n",
       "2184765             kaliman's          3\n",
       "2184766             pagagawan          3\n",
       "2184767             stanfilco          3\n",
       "2184768            formelleza          3\n",
       "2184769           sikad-sikad          3\n",
       "2184770                 uraro          3\n",
       "2184771               j'viens          3\n",
       "2184772              leytenos          3\n",
       "2184773         extra-burghal          3\n",
       "2184774  fürstenberg-blomberg          3\n",
       "2184775            al-ikhshīd          3\n",
       "2184776                   石碣村          3\n",
       "2184777   non-number-specific          3\n",
       "2184778      salon-bolshevist          3\n",
       "2184779                 fi-ta          3"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[-50:] # LOOOOL !!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "a92cd44a",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "56708"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Remove stop words from the occurence dataframe \n",
    "\n",
    "for i in range(len(stop_words)):\n",
    "    try :\n",
    "        df.drop([(df[df[\"words\"] == stop_words[i]]).index[0]], inplace=True)\n",
    "    except :\n",
    "        continue\n",
    "        \n",
    "df.reset_index(drop=True, inplace=True)\n",
    "df_occu = df\n",
    "\n",
    "len(df_occu)\n",
    "\n",
    "df_occu.to_csv(\"cleaned_english_words_occurence.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55fba626",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Preprocessing Vocabulary English/French"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "cfd25cef",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# vocabulary processing containing : english, french\n",
    "\n",
    "vocabulary = pd.read_excel(\"Vocabulary.xlsx\")\n",
    "\n",
    "#ici on tri en faisant du preprocessing + en enlevant les dupliqués exacts\n",
    "\n",
    "def remove_punctuation(text):\n",
    "    special_punc = '''!()-[]{};:'\"\\,<>./?@#$%^&*_~'''\n",
    "    for ele in text:\n",
    "        if ele in special_punc:\n",
    "            text = text.replace(ele, \"\")\n",
    "    return text\n",
    "\n",
    "def remove_stopwords(text):\n",
    "    text_tokens = word_tokenize(text)\n",
    "    tokens_without_sw = ' '.join([word for word in text_tokens if not word in stopwords.words()])\n",
    "    return tokens_without_sw\n",
    "\n",
    "def remove_empty(text):\n",
    "    return \" \".join(text.strip().split())\n",
    "\n",
    "def preprocessing_lower_punc_sw_stem(df, col_name):\n",
    "    \n",
    "    #Preprocessing (\" \",lower,punctuation,stop_words,stemming)\n",
    "    df[col_name]=df[col_name].apply(lambda x: remove_empty(x))\n",
    "    df[col_name]=df[col_name].apply(lambda x: x.lower())\n",
    "    \n",
    "    porter = PorterStemmer()\n",
    "    df[col_name + '_nopunct']=df[col_name].apply(lambda x: remove_punctuation(x))\n",
    "    df[col_name + '_noswords']=df[col_name + '_nopunct'].apply(lambda x: remove_stopwords(x))\n",
    "    df[col_name + '_stem']=df[col_name + '_noswords'].apply(lambda x: porter.stem(x))\n",
    "\n",
    "    #Duplicates\n",
    "    df=df.drop_duplicates(subset=[col_name])\n",
    "    df=df.drop_duplicates(subset=[col_name + '_stem'])\n",
    "    df=df.reset_index(drop=True)\n",
    "    \n",
    "    \n",
    "for col_name in [\"ENGLISH\", \"FRENCH\"]:\n",
    "    preprocessing_lower_punc_sw_stem(vocabulary, col_name)\n",
    "    \n",
    "# vocabulary.to_csv(\"clean_vocabulary.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "89190ad6",
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>FRENCH</th>\n",
       "      <th>ENGLISH</th>\n",
       "      <th>THEME</th>\n",
       "      <th>IMPORTANT</th>\n",
       "      <th>ENGLISH_nopunct</th>\n",
       "      <th>ENGLISH_noswords</th>\n",
       "      <th>ENGLISH_stem</th>\n",
       "      <th>FRENCH_nopunct</th>\n",
       "      <th>FRENCH_noswords</th>\n",
       "      <th>FRENCH_stem</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>accepter</td>\n",
       "      <td>to accept</td>\n",
       "      <td>verbes a connaitre en</td>\n",
       "      <td>Non</td>\n",
       "      <td>to accept</td>\n",
       "      <td>accept</td>\n",
       "      <td>accept</td>\n",
       "      <td>accepter</td>\n",
       "      <td>accepter</td>\n",
       "      <td>accept</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>acheter</td>\n",
       "      <td>to buy</td>\n",
       "      <td>verbes a connaitre en</td>\n",
       "      <td>Non</td>\n",
       "      <td>to buy</td>\n",
       "      <td>buy</td>\n",
       "      <td>buy</td>\n",
       "      <td>acheter</td>\n",
       "      <td>acheter</td>\n",
       "      <td>achet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>aider</td>\n",
       "      <td>to help</td>\n",
       "      <td>verbes a connaitre en</td>\n",
       "      <td>Non</td>\n",
       "      <td>to help</td>\n",
       "      <td>help</td>\n",
       "      <td>help</td>\n",
       "      <td>aider</td>\n",
       "      <td>aider</td>\n",
       "      <td>aider</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>aimer</td>\n",
       "      <td>to like, to love</td>\n",
       "      <td>verbes a connaitre en</td>\n",
       "      <td>Non</td>\n",
       "      <td>to like to love</td>\n",
       "      <td>like love</td>\n",
       "      <td>like lov</td>\n",
       "      <td>aimer</td>\n",
       "      <td>aimer</td>\n",
       "      <td>aimer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ajouter</td>\n",
       "      <td>to add</td>\n",
       "      <td>verbes a connaitre en</td>\n",
       "      <td>Non</td>\n",
       "      <td>to add</td>\n",
       "      <td>add</td>\n",
       "      <td>add</td>\n",
       "      <td>ajouter</td>\n",
       "      <td>ajouter</td>\n",
       "      <td>ajout</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14015</th>\n",
       "      <td>se rétablir de</td>\n",
       "      <td>to get over stg, to recover from</td>\n",
       "      <td>covid coronavirus</td>\n",
       "      <td>NaN</td>\n",
       "      <td>to get over stg to recover from</td>\n",
       "      <td>get stg recover</td>\n",
       "      <td>get stg recov</td>\n",
       "      <td>se rétablir de</td>\n",
       "      <td>rétablir</td>\n",
       "      <td>rétablir</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14016</th>\n",
       "      <td>soigner</td>\n",
       "      <td>to treat</td>\n",
       "      <td>covid coronavirus</td>\n",
       "      <td>NaN</td>\n",
       "      <td>to treat</td>\n",
       "      <td>treat</td>\n",
       "      <td>treat</td>\n",
       "      <td>soigner</td>\n",
       "      <td>soigner</td>\n",
       "      <td>soigner</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14017</th>\n",
       "      <td>tester les patients</td>\n",
       "      <td>to test patients</td>\n",
       "      <td>covid coronavirus</td>\n",
       "      <td>NaN</td>\n",
       "      <td>to test patients</td>\n",
       "      <td>test patients</td>\n",
       "      <td>test pati</td>\n",
       "      <td>tester les patients</td>\n",
       "      <td>tester patients</td>\n",
       "      <td>tester pati</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14018</th>\n",
       "      <td>tomber malade</td>\n",
       "      <td>to be taken ill</td>\n",
       "      <td>covid coronavirus</td>\n",
       "      <td>NaN</td>\n",
       "      <td>to be taken ill</td>\n",
       "      <td>taken</td>\n",
       "      <td>taken</td>\n",
       "      <td>tomber malade</td>\n",
       "      <td>tomber malade</td>\n",
       "      <td>tomber malad</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14019</th>\n",
       "      <td>transmettre une maladie</td>\n",
       "      <td>to transmit a disease</td>\n",
       "      <td>covid coronavirus</td>\n",
       "      <td>NaN</td>\n",
       "      <td>to transmit a disease</td>\n",
       "      <td>transmit disease</td>\n",
       "      <td>transmit diseas</td>\n",
       "      <td>transmettre une maladie</td>\n",
       "      <td>transmettre maladie</td>\n",
       "      <td>transmettre maladi</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>14020 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                        FRENCH                           ENGLISH  \\\n",
       "0                     accepter                         to accept   \n",
       "1                      acheter                            to buy   \n",
       "2                        aider                           to help   \n",
       "3                        aimer                  to like, to love   \n",
       "4                      ajouter                            to add   \n",
       "...                        ...                               ...   \n",
       "14015           se rétablir de  to get over stg, to recover from   \n",
       "14016                  soigner                          to treat   \n",
       "14017      tester les patients                  to test patients   \n",
       "14018            tomber malade                   to be taken ill   \n",
       "14019  transmettre une maladie             to transmit a disease   \n",
       "\n",
       "                       THEME IMPORTANT                  ENGLISH_nopunct  \\\n",
       "0      verbes a connaitre en       Non                        to accept   \n",
       "1      verbes a connaitre en       Non                           to buy   \n",
       "2      verbes a connaitre en       Non                          to help   \n",
       "3      verbes a connaitre en       Non                  to like to love   \n",
       "4      verbes a connaitre en       Non                           to add   \n",
       "...                      ...       ...                              ...   \n",
       "14015      covid coronavirus       NaN  to get over stg to recover from   \n",
       "14016      covid coronavirus       NaN                         to treat   \n",
       "14017      covid coronavirus       NaN                 to test patients   \n",
       "14018      covid coronavirus       NaN                  to be taken ill   \n",
       "14019      covid coronavirus       NaN            to transmit a disease   \n",
       "\n",
       "       ENGLISH_noswords     ENGLISH_stem           FRENCH_nopunct  \\\n",
       "0                accept           accept                 accepter   \n",
       "1                   buy              buy                  acheter   \n",
       "2                  help             help                    aider   \n",
       "3             like love         like lov                    aimer   \n",
       "4                   add              add                  ajouter   \n",
       "...                 ...              ...                      ...   \n",
       "14015   get stg recover    get stg recov           se rétablir de   \n",
       "14016             treat            treat                  soigner   \n",
       "14017     test patients        test pati      tester les patients   \n",
       "14018             taken            taken            tomber malade   \n",
       "14019  transmit disease  transmit diseas  transmettre une maladie   \n",
       "\n",
       "           FRENCH_noswords         FRENCH_stem  \n",
       "0                 accepter              accept  \n",
       "1                  acheter               achet  \n",
       "2                    aider               aider  \n",
       "3                    aimer               aimer  \n",
       "4                  ajouter               ajout  \n",
       "...                    ...                 ...  \n",
       "14015             rétablir            rétablir  \n",
       "14016              soigner             soigner  \n",
       "14017      tester patients         tester pati  \n",
       "14018        tomber malade        tomber malad  \n",
       "14019  transmettre maladie  transmettre maladi  \n",
       "\n",
       "[14020 rows x 10 columns]"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "a04e407c",
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\JOSEPH~1.PIC\\AppData\\Local\\Temp/ipykernel_34092/3349348133.py:23: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_voc2['Nearest_Neighbor']=df_voc2.iloc[id_nearest[:,1]]['ENGLISH'].tolist()\n",
      "C:\\Users\\JOSEPH~1.PIC\\AppData\\Local\\Temp/ipykernel_34092/3349348133.py:24: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_voc2['Proche_Voisin']=df_voc2.iloc[id_nearest[:,1]]['FRENCH'].tolist()\n",
      "C:\\Users\\JOSEPH~1.PIC\\AppData\\Local\\Temp/ipykernel_34092/3349348133.py:25: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_voc2['Distance_Nearest']=distance_nearest[:,1].tolist()\n",
      "C:\\Users\\JOSEPH~1.PIC\\AppData\\Local\\Temp/ipykernel_34092/3349348133.py:26: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_voc2['ID_Nearest_Neighbor']=id_nearest[:,1]\n"
     ]
    }
   ],
   "source": [
    "# Deduplication avec BallTree\n",
    "\n",
    "# from nltk.stem import LancasterStemmer\n",
    "from sklearn.neighbors import BallTree\n",
    "# from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "\n",
    "#ici on essaye d'enlever les \"almost duplicate\" en vectorisant + utilisant BallTree sur les mots\n",
    "#autrement dit on crée les colonnes \"ID_Nearest_Neighbor\" et \"Distance_Nearest\" sur un échantillon de 2000 lignes\n",
    "sample_voc = vocabulary[0:2000]['ENGLISH_stem'].tolist()\n",
    "vectorizer = TfidfVectorizer()\n",
    "matrix = vectorizer.fit_transform(sample_voc).todense()\n",
    "matrix = pd.DataFrame(matrix, columns=vectorizer.get_feature_names_out ())\n",
    "top_words = matrix.sum(axis=0).sort_values(ascending=False)\n",
    "\n",
    "tree = BallTree(matrix.values, leaf_size=2)\n",
    "\n",
    "distance_nearest, id_nearest = tree.query(\n",
    "    matrix.values,\n",
    "    k=3,)\n",
    "\n",
    "df_voc2=vocabulary[0:2000]\n",
    "df_voc2['Nearest_Neighbor']=df_voc2.iloc[id_nearest[:,1]]['ENGLISH'].tolist()\n",
    "df_voc2['Proche_Voisin']=df_voc2.iloc[id_nearest[:,1]]['FRENCH'].tolist()\n",
    "df_voc2['Distance_Nearest']=distance_nearest[:,1].tolist()\n",
    "df_voc2['ID_Nearest_Neighbor']=id_nearest[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "537f3526",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# ici on delete les mots trop proche les uns des autres grâce au BallTree (dist<0.6)\n",
    "print(len(df_voc2[df_voc2.Distance_Nearest<0.6]))\n",
    "ids_todelete=df_voc2[df_voc2.Distance_Nearest<0.6].index.tolist()\n",
    "ids_close=df_voc2[df_voc2.Distance_Nearest<0.6]['ID_Nearest_Neighbor'].tolist()\n",
    "no_delete_ids=[]\n",
    "for count,id_todelete in enumerate(ids_todelete):\n",
    "    if id_todelete not in no_delete_ids:\n",
    "        df_voc2.drop([id_todelete],inplace=True)\n",
    "        no_delete_ids.append(ids_close[count])\n",
    "    else: \n",
    "        pass\n",
    "print(len(df_voc2[df_voc2.Distance_Nearest<0.6]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2a9fc87",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Fusion the two databases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "ce3bd04b",
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>FRENCH</th>\n",
       "      <th>ENGLISH</th>\n",
       "      <th>THEME</th>\n",
       "      <th>IMPORTANT</th>\n",
       "      <th>ENGLISH_nopunct</th>\n",
       "      <th>ENGLISH_noswords</th>\n",
       "      <th>ENGLISH_stem</th>\n",
       "      <th>FRENCH_nopunct</th>\n",
       "      <th>FRENCH_noswords</th>\n",
       "      <th>FRENCH_stem</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>accepter</td>\n",
       "      <td>to accept</td>\n",
       "      <td>verbes a connaitre en</td>\n",
       "      <td>Non</td>\n",
       "      <td>to accept</td>\n",
       "      <td>accept</td>\n",
       "      <td>accept</td>\n",
       "      <td>accepter</td>\n",
       "      <td>accepter</td>\n",
       "      <td>accept</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>acheter</td>\n",
       "      <td>to buy</td>\n",
       "      <td>verbes a connaitre en</td>\n",
       "      <td>Non</td>\n",
       "      <td>to buy</td>\n",
       "      <td>buy</td>\n",
       "      <td>buy</td>\n",
       "      <td>acheter</td>\n",
       "      <td>acheter</td>\n",
       "      <td>achet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>aider</td>\n",
       "      <td>to help</td>\n",
       "      <td>verbes a connaitre en</td>\n",
       "      <td>Non</td>\n",
       "      <td>to help</td>\n",
       "      <td>help</td>\n",
       "      <td>help</td>\n",
       "      <td>aider</td>\n",
       "      <td>aider</td>\n",
       "      <td>aider</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>aimer</td>\n",
       "      <td>to like, to love</td>\n",
       "      <td>verbes a connaitre en</td>\n",
       "      <td>Non</td>\n",
       "      <td>to like to love</td>\n",
       "      <td>like love</td>\n",
       "      <td>like lov</td>\n",
       "      <td>aimer</td>\n",
       "      <td>aimer</td>\n",
       "      <td>aimer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ajouter</td>\n",
       "      <td>to add</td>\n",
       "      <td>verbes a connaitre en</td>\n",
       "      <td>Non</td>\n",
       "      <td>to add</td>\n",
       "      <td>add</td>\n",
       "      <td>add</td>\n",
       "      <td>ajouter</td>\n",
       "      <td>ajouter</td>\n",
       "      <td>ajout</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14015</th>\n",
       "      <td>se rétablir de</td>\n",
       "      <td>to get over stg, to recover from</td>\n",
       "      <td>covid coronavirus</td>\n",
       "      <td>NaN</td>\n",
       "      <td>to get over stg to recover from</td>\n",
       "      <td>get stg recover</td>\n",
       "      <td>get stg recov</td>\n",
       "      <td>se rétablir de</td>\n",
       "      <td>rétablir</td>\n",
       "      <td>rétablir</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14016</th>\n",
       "      <td>soigner</td>\n",
       "      <td>to treat</td>\n",
       "      <td>covid coronavirus</td>\n",
       "      <td>NaN</td>\n",
       "      <td>to treat</td>\n",
       "      <td>treat</td>\n",
       "      <td>treat</td>\n",
       "      <td>soigner</td>\n",
       "      <td>soigner</td>\n",
       "      <td>soigner</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14017</th>\n",
       "      <td>tester les patients</td>\n",
       "      <td>to test patients</td>\n",
       "      <td>covid coronavirus</td>\n",
       "      <td>NaN</td>\n",
       "      <td>to test patients</td>\n",
       "      <td>test patients</td>\n",
       "      <td>test pati</td>\n",
       "      <td>tester les patients</td>\n",
       "      <td>tester patients</td>\n",
       "      <td>tester pati</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14018</th>\n",
       "      <td>tomber malade</td>\n",
       "      <td>to be taken ill</td>\n",
       "      <td>covid coronavirus</td>\n",
       "      <td>NaN</td>\n",
       "      <td>to be taken ill</td>\n",
       "      <td>taken</td>\n",
       "      <td>taken</td>\n",
       "      <td>tomber malade</td>\n",
       "      <td>tomber malade</td>\n",
       "      <td>tomber malad</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14019</th>\n",
       "      <td>transmettre une maladie</td>\n",
       "      <td>to transmit a disease</td>\n",
       "      <td>covid coronavirus</td>\n",
       "      <td>NaN</td>\n",
       "      <td>to transmit a disease</td>\n",
       "      <td>transmit disease</td>\n",
       "      <td>transmit diseas</td>\n",
       "      <td>transmettre une maladie</td>\n",
       "      <td>transmettre maladie</td>\n",
       "      <td>transmettre maladi</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>14020 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                        FRENCH                           ENGLISH  \\\n",
       "0                     accepter                         to accept   \n",
       "1                      acheter                            to buy   \n",
       "2                        aider                           to help   \n",
       "3                        aimer                  to like, to love   \n",
       "4                      ajouter                            to add   \n",
       "...                        ...                               ...   \n",
       "14015           se rétablir de  to get over stg, to recover from   \n",
       "14016                  soigner                          to treat   \n",
       "14017      tester les patients                  to test patients   \n",
       "14018            tomber malade                   to be taken ill   \n",
       "14019  transmettre une maladie             to transmit a disease   \n",
       "\n",
       "                       THEME IMPORTANT                  ENGLISH_nopunct  \\\n",
       "0      verbes a connaitre en       Non                        to accept   \n",
       "1      verbes a connaitre en       Non                           to buy   \n",
       "2      verbes a connaitre en       Non                          to help   \n",
       "3      verbes a connaitre en       Non                  to like to love   \n",
       "4      verbes a connaitre en       Non                           to add   \n",
       "...                      ...       ...                              ...   \n",
       "14015      covid coronavirus       NaN  to get over stg to recover from   \n",
       "14016      covid coronavirus       NaN                         to treat   \n",
       "14017      covid coronavirus       NaN                 to test patients   \n",
       "14018      covid coronavirus       NaN                  to be taken ill   \n",
       "14019      covid coronavirus       NaN            to transmit a disease   \n",
       "\n",
       "       ENGLISH_noswords     ENGLISH_stem           FRENCH_nopunct  \\\n",
       "0                accept           accept                 accepter   \n",
       "1                   buy              buy                  acheter   \n",
       "2                  help             help                    aider   \n",
       "3             like love         like lov                    aimer   \n",
       "4                   add              add                  ajouter   \n",
       "...                 ...              ...                      ...   \n",
       "14015   get stg recover    get stg recov           se rétablir de   \n",
       "14016             treat            treat                  soigner   \n",
       "14017     test patients        test pati      tester les patients   \n",
       "14018             taken            taken            tomber malade   \n",
       "14019  transmit disease  transmit diseas  transmettre une maladie   \n",
       "\n",
       "           FRENCH_noswords         FRENCH_stem  \n",
       "0                 accepter              accept  \n",
       "1                  acheter               achet  \n",
       "2                    aider               aider  \n",
       "3                    aimer               aimer  \n",
       "4                  ajouter               ajout  \n",
       "...                    ...                 ...  \n",
       "14015             rétablir            rétablir  \n",
       "14016              soigner             soigner  \n",
       "14017      tester patients         tester pati  \n",
       "14018        tomber malade        tomber malad  \n",
       "14019  transmettre maladie  transmettre maladi  \n",
       "\n",
       "[14020 rows x 10 columns]"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "39cd881c",
   "metadata": {
    "hidden": true,
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>words</th>\n",
       "      <th>occurences</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>also</td>\n",
       "      <td>5450043</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>first</td>\n",
       "      <td>4840311</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>one</td>\n",
       "      <td>4151416</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>new</td>\n",
       "      <td>4039430</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>two</td>\n",
       "      <td>3415323</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56703</th>\n",
       "      <td>outstep</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56704</th>\n",
       "      <td>rescans</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56705</th>\n",
       "      <td>unbelieved</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56706</th>\n",
       "      <td>unclasping</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56707</th>\n",
       "      <td>gallivanted</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>56708 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             words  occurences\n",
       "0             also     5450043\n",
       "1            first     4840311\n",
       "2              one     4151416\n",
       "3              new     4039430\n",
       "4              two     3415323\n",
       "...            ...         ...\n",
       "56703      outstep           3\n",
       "56704      rescans           3\n",
       "56705   unbelieved           3\n",
       "56706   unclasping           3\n",
       "56707  gallivanted           3\n",
       "\n",
       "[56708 rows x 2 columns]"
      ]
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleaned_english_words_occurence = pd.read_csv(\"cleaned_english_words_occurence.csv\", index_col=0)\n",
    "cleaned_english_words_occurence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0829b3f",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "sample_voc = vocabulary[0:2000]['ENGLISH_stem'].tolist()\n",
    "vectorizer = TfidfVectorizer()\n",
    "matrix = vectorizer.fit_transform(sample_voc).todense()\n",
    "matrix = pd.DataFrame(matrix, columns=vectorizer.get_feature_names_out ())\n",
    "top_words = matrix.sum(axis=0).sort_values(ascending=False)\n",
    "\n",
    "tree = BallTree(matrix.values, leaf_size=2)\n",
    "\n",
    "distance_nearest, id_nearest = tree.query(\n",
    "    matrix.values,\n",
    "    k=3,)\n",
    "\n",
    "df_voc2=vocabulary[0:2000]\n",
    "df_voc2['Nearest_Neighbor']=df_voc2.iloc[id_nearest[:,1]]['ENGLISH'].tolist()\n",
    "df_voc2['Proche_Voisin']=df_voc2.iloc[id_nearest[:,1]]['FRENCH'].tolist()\n",
    "df_voc2['Distance_Nearest']=distance_nearest[:,1].tolist()\n",
    "df_voc2['ID_Nearest_Neighbor']=id_nearest[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "a3606cd1",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "vocabulary.to_csv(\"clean_vocabulary.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac4f3010",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "f58a051e",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>words</th>\n",
       "      <th>occurences</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>also</td>\n",
       "      <td>5450043</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>first</td>\n",
       "      <td>4840311</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>one</td>\n",
       "      <td>4151416</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>new</td>\n",
       "      <td>4039430</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>two</td>\n",
       "      <td>3415323</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56703</th>\n",
       "      <td>outstep</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56704</th>\n",
       "      <td>rescans</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56705</th>\n",
       "      <td>unbelieved</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56706</th>\n",
       "      <td>unclasping</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56707</th>\n",
       "      <td>gallivanted</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>56708 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             words  occurences\n",
       "0             also     5450043\n",
       "1            first     4840311\n",
       "2              one     4151416\n",
       "3              new     4039430\n",
       "4              two     3415323\n",
       "...            ...         ...\n",
       "56703      outstep           3\n",
       "56704      rescans           3\n",
       "56705   unbelieved           3\n",
       "56706   unclasping           3\n",
       "56707  gallivanted           3\n",
       "\n",
       "[56708 rows x 2 columns]"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abafc44c",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "cleaned_english_words_occurence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ebca451d",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "french_column = 'french'\n",
    "english_column = 'english'\n",
    "stem_colmun = 'stem'\n",
    "\n",
    "df_french_english = pd.DataFrame(columns=[french_column, english_column, stem_colmun])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7192c0e",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df_french_english.loc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e73a9c0",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "7251c4ef",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>french</th>\n",
       "      <th>english</th>\n",
       "      <th>stem</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [french, english, stem]\n",
       "Index: []"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_french_english"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60a1f8db",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df.to_excel(\"english_frequency_list.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3db14f41",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
